# Copyright 2025 The android_world Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""T3A: Text-only Autonomous Agent for Android."""

from android_world.agents import agent_utils
from android_world.agents import base_agent
from android_world.agents import infer
from android_world.agents import m3a_utils
from android_world.env import adb_utils
from android_world.env import interface
from android_world.env import json_action
from android_world.env import representation_utils

from android_world.agents.vimo import vimo_sum, vimo_reward, vimo_reward_t2
import time

import os

from PIL import Image
import re
import json
from typing import Optional, Any, Dict
import io
import requests
import numpy as np
import concurrent.futures
import ast

# 奖励值agent，给定目标，操作历史，action，语义描述，操作前ui元素描述，操作前后的gui图像，agent将判断该action是否有效，并给出原因以及置信度，置信度作为奖励值


def post_figure_action(image_array, action, url):
    """Helper function to post a single figure-action pair."""
    # Convert NumPy array to PNG in memory
    pil_image = Image.fromarray(image_array)
    buffer = io.BytesIO()
    pil_image.save(buffer, format='PNG')
    buffer.seek(0)

    files = {
        "file": ("test.png", buffer, "image/png")
    }
    data = {
        "action": action
    }

    response = requests.post(url, files=files, data=data)
    returned_image_bytes = io.BytesIO(response.content)
    returned_image = Image.open(returned_image_bytes)
    returned_array = np.array(returned_image)
    
    return returned_array


PROMPT_PREFIX = (
    "You are a reward-scoring agent whose sole responsibility is to evaluate"
    " whether a candidate action (performed on an Android phone) is helpful"
    " for achieving the user's overall goal. You do NOT execute actions,"
    " nor should you output any action. Instead, using the provided goal,"
    " action (and its semantic summary), history, UI element descriptions, and"
    " before/after screenshots, you must judge the action's validity and give"
    " a reasoned confidence score.\n\n"
    "Use the following information to form your judgment:\n"
    "- The overall user goal/request.\n"
    "- A chronological history of actions already taken.\n"
    "- The candidate action for the latest step and its semantic summary.\n"
    "- The UI state before the action (detailed element descriptions).\n"
    "- The predicted UI state after the action (generated by a world model).\n\n"
    "For your reference only (do NOT output actions): here is the list of"
    " common action types and their JSON formats. This list is provided so you"
    " can correctly interpret what the candidate action means semantically."
    " Do NOT produce these actions as your output — you should only produce a"
    " judgment (see template below):\n"
    '- status: `{{"action_type": "status", "goal_status": "complete" or "infeasible"}}`\n'
    '- answer: `{{"action_type": "answer", "text": "<answer_text>"}}`\n'
    '- click/tap: `{{"action_type": "click", "index": <target_index>}}`.\n'
    '- long press: `{{"action_type": "long_press", "index": <target_index>}}`.\n'
    '- input text: `{{"action_type": "input_text", "text": <text_input>, "index": <target_index>}}`\n'
    '- press enter: `{{"action_type": "keyboard_enter"}}`\n'
    '- navigate home: `{{"action_type": "navigate_home"}}`\n'
    '- navigate back: `{{"action_type": "navigate_back"}}`\n'
    '- scroll: `{{"action_type": "scroll", "direction": <up, down, left, right>, "index": <optional_target_index>}}`\n'
    '- open app: `{{"action_type": "open_app", "app_name": <name>}}`\n'
    '- wait for update: `{{"action_type": "wait"}}`\n\n'
)

REWARD_PROMPT_TEMPLATE = (
    PROMPT_PREFIX
    + '\nThe overall user goal/request is: {goal}\n\n'
    'Here is a history of what you have done so far:\n{history}\n\n'
    # sum是vimo_sum中的agent概括出来的该action的用户行为描述
    'This is the action you picked in the latest step: {action}, whose semantic description is: {sum}\n'
    'Your goal is to judge **whether the action you picked in the latest step'
    ' is on the right track to the successful execution of the overall user goal/request**.\n'
    # 这里介绍world model产生的gui预测图像
    "You will be given the screenshots before and after you performed the action\n"
    '- The first screenshot corresponds to the UI state before you performed the action.\n'
    '- The second screenshot corresponds to the UI state after you performed the action.\n'
    ' Also here is the list of detailed information for some UI elements'
    ' in the before screenshot:\n{before_elements}\n'
    ' Note that, the "after" screenshot is generated by the agent’s world model.'
    ' As such, it may not faithfully represent the real UI. For instance: Some UI elements in the simulated "after" screenshot may not exist in a real UI.'
    ' Your evaluation should consider the reliability of the UI predictions. If the "after" screenshot contains unreasonable elements, this likely indicates a failure.\n'
    + '\n\nNow provide your judgment on the selected action in JSON format. Your response must include:\n'
    'Reason: A detailed explanation of why the action is valid or invalid.\n'
    'Judgment: Your judgment must be either "valid" or "invalid".\n' 
    'Confidence: A confidence score between 0.0 and 1.0, reflecting how likely your judgment is correct. Use this scale:\n'
    '- 1.0: Absolute certainty based on clear evidence or explicit rules\n'
    '- 0.8-0.9: High confidence with strong supporting evidence\n'
    '- 0.6-0.7: Moderate confidence with some ambiguity\n'
    '- 0.4-0.5: Low confidence due to significant uncertainty\n'
    '- 0.1-0.3: Very low confidence with minimal supporting evidence\n\n'
    'You must follow this structure exactly in pure Json format without any comment or code block:\n'
    '[{{"Reason": "...", "Judgement": "valid" or "invalid", "Confidence": a score between 0.0 and 1.0}}] \n'
    'Your Answer:\n'
)


# # 详细描述了代理作为Android手机操作者的角色，以及它可以执行的有限的各种动作action_type，如点击、长按、输入文本、导航等，并定义了各自需要的返回json格式
# PROMPT_PREFIX = (
#     'You are an agent who can operate an Android phone on behalf of a user.'
#     " Based on user's goal/request, you may\n"
#     '- Answer back if the request/goal is a question (or a chat message), like'
#     ' user asks "What is my schedule for today?".\n'
#     '- Complete some tasks described in the requests/goals by performing'
#     ' actions (step by step) on the phone.\n\n'
#     'When given a user request, you will try to complete it step by step. At'
#     ' each step, a list of descriptions for most UI elements on the'
#     ' current screen will be given to you (each element can be specified by an'
#     ' index), together with a history of what you have done in previous steps.'
#     ' Based on these pieces of information and the goal, you must choose to'
#     ' perform one of the action in the following list (action description'
#     ' followed by the JSON format) by outputing the action in the correct JSON'
#     ' format.\n'
#     '- If you think the task has been completed, finish the task by using the'
#     ' status action with complete as goal_status:'
#     ' `{{"action_type": "status", "goal_status": "complete"}}`\n'
#     '- If you think the task is not'
#     " feasible (including cases like you don't have enough information or can"
#     ' not perform some necessary actions), finish by using the `status` action'
#     ' with infeasible as goal_status:'
#     ' `{{"action_type": "status", "goal_status": "infeasible"}}`\n'
#     "- Answer user's question:"
#     ' `{{"action_type": "answer", "text": "<answer_text>"}}`\n'
#     '- Click/tap on a UI element (specified by its index) on the screen:'
#     ' `{{"action_type": "click", "index": <target_index>}}`.\n'
#     '- Long press on a UI element (specified by its index) on the screen:'
#     ' `{{"action_type": "long_press", "index": <target_index>}}`.\n'
#     '- Type text into an editable text field (specified by its index), this'
#     ' action contains clicking the text field, typing in the text and pressing'
#     ' the enter, so no need to click on the target field to start:'
#     ' `{{"action_type": "input_text", "text": <text_input>, "index":'
#     ' <target_index>}}`\n'
#     '- Press the Enter key: `{{"action_type": "keyboard_enter"}}`\n'
#     '- Navigate to the home screen: `{{"action_type": "navigate_home"}}`\n'
#     '- Navigate back: `{{"action_type": "navigate_back"}}`\n'
#     '- Scroll the screen or a scrollable UI element in one of the four'
#     ' directions, use the same numeric index as above if you want to scroll a'
#     ' specific UI element, leave it empty when scroll the whole screen:'
#     ' `{{"action_type": "scroll", "direction": <up, down, left, right>,'
#     ' "index": <optional_target_index>}}`\n'
#     '- Open an app (nothing will happen if the app is not installed):'
#     ' `{{"action_type": "open_app", "app_name": <name>}}`\n'
#     '- Wait for the screen to update: `{{"action_type": "wait"}}`\n'
# )



# # 奖励提示模板，给定单个action，对应的world model预测出来的gui图像和其他信息，要求agent判断行为是否有效，对其打分并给出理由
# REWARD_PROMPT_TEMPLATE = (
#     PROMPT_PREFIX
#     + '\nThe overall user goal/request is: {goal}\n\n'
#     'Here is a history of what you have done so far:\n{history}\n\n'
#     # sum是vimo_sum中的agent概括出来的该action的用户行为描述
#     'This is the action you picked in the latest step: {action}, whose semantic description is: {sum}\n'
#     'Your goal is to judge **whether the action you picked in the latest step'
#     ' is on the right track to the successful execution of the overall user goal/request**.\n'
#     # 这里介绍world model产生的gui预测图像
#     "You will be given the screenshots before and after you performed the action\n"
#     '- The first screenshot corresponds to the UI state before you performed the action.\n'
#     '- The second screenshot corresponds to the UI state after you performed the action.\n'
#     ' Also here is the list of detailed information for some UI elements'
#     ' in the before screenshot:\n{before_elements}\n'
#     ' Note that, the "after" screenshot is generated by the agent’s world model.'
#     ' As such, it may not faithfully represent the real UI. For instance: Some UI elements in the simulated "after" screenshot may not exist in a real UI.'
#     ' Your evaluation should consider the reliability of the UI predictions. If the "after" screenshot contains unreasonable elements, this likely indicates a failure.\n'
#     + '\n\nNow provide your judgment on the selected action in JSON format. Your response must include:\n'
#     'Reason: A detailed explanation of why the action is valid or invalid.\n'
#     'Judgment: Your judgment must be either "valid" or "invalid".\n' 
#     'Confidence: A confidence score between 0.0 and 1.0, reflecting how likely your judgment is correct. Use this scale:\n'
#     '- 1.0: Absolute certainty based on clear evidence or explicit rules\n'
#     '- 0.8-0.9: High confidence with strong supporting evidence\n'
#     '- 0.6-0.7: Moderate confidence with some ambiguity\n'
#     '- 0.4-0.5: Low confidence due to significant uncertainty\n'
#     '- 0.1-0.3: Very low confidence with minimal supporting evidence\n\n'
#     'You must follow this structure exactly in pure Json format without any comment or code block:\n'
#     '[{{"Reason": "...", "Judgement": "valid" or "invalid", "Confidence": a score between 0.0 and 1.0}}] \n'
#     'Your Answer:\n'
# )

def _reward_prompt(
    action, 
    act_re, 
    low_level_ins,
    history: list[str],
    goal: str,
    before_elements: str,
) -> str:
  """Generate the prompt for the summarization step.

  Args:
    action: Action picked.
    reason: The reason to pick the action.
    goal: The overall goal.
    before_elements: Information for UI elements on the before screenshot.
    after_elements: Information for UI elements on the after screenshot.

  Returns:
    The text prompt for summarization that will be sent to gpt4v.
  """
  if history:
    history = '\n'.join(history)
  else:
    history = 'You just started, no action has been performed yet.'
  return REWARD_PROMPT_TEMPLATE.format(
      goal=goal,
      action=action,
      sum=low_level_ins,
      history=history,
      before_elements=before_elements,
  )

def extratc_json(output):
    output_u = {}

    # Try loading directly
    try:
        output = json.loads(output)
        return output[0]
    except (json.JSONDecodeError, TypeError):
        pass

    # Try extracting JSON from triple backticks with optional language
    match = re.search(r"```(?:json)?\s*(\[\s*{.*?}\s*])\s*```", output, re.DOTALL)
    if match:
        json_str = match.group(1)
    else:
        # Try to fallback to JSON-like string in case no code block is matched
        json_str = output.strip()

    # Attempt to parse JSON
    try:
        parsed = json.loads(json_str)
        return parsed[0] if isinstance(parsed, list) and len(parsed) > 0 else None
    except json.JSONDecodeError as e:
        print(json_str)
        print(e)
        return None
      
class T3A(base_agent.EnvironmentInteractingAgent):
  """Text only autonomous agent for Android."""

  def __init__(
      self,
      env,
      llm: infer.LlmWrapper,
      name: str = 'T3A',
  ):
    """Initializes a RandomAgent.

    Args:
      env: The environment.
      llm: The text only LLM.
      name: The agent name.
    """
    super().__init__(env, name)
    self.llm = llm
    self.history = []
    self.additional_guidelines = None

  def step(self, goal, his, action, act_re, low_level_ins, before_element_list, image_array_before, image_array_after) -> base_agent.AgentInteractionResult:
    action_prompt = _reward_prompt(
        action, 
        act_re, 
        low_level_ins,
        [
            'Step ' + str(i + 1) + ': ' + step_info['summary']
            for i, step_info in enumerate(his)
        ],
        goal,
        before_element_list,
    )
    ################ 输入reward模板和action前后的gui变化，输出action的reward判断 ################
    action_output, is_safe, raw_response = self.llm.predict_mm(
            action_prompt,
            [
                image_array_before,
                image_array_after,
            ]
        )
    action_output_json = extratc_json(action_output)
    print("打分模型给出的判断为：", action_output_json)
    reason, judge, confidence = action_output_json['Reason'], action_output_json['Judgement'], action_output_json['Confidence']
    judge = judge.lower()
    if(judge =='invalid'):
        return (-1.0 * confidence)
    elif(judge =='valid'):
        return (1.0 * confidence)
    
    
